---
title: "Project Report"
output: 
  html_document:
    highlight: tango
date: "2024-12-15"
Group3: "Julian Janisch, Hanka Jankovicova, Livia Miclaus, Sofiia Selimova, Bence Zsolt Vízvári"
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

**Introduction:**

For our project we have chosen data set of Adidas from North America. We
want to analyze it and predict which factors are the strongest
predictors of profitability for Adidas products, which sales channel is
the most effective for each product type in terms of sales and
profitability and how do different factors influence the number of units
sold. The insights will help Adidas to make better decisions about
pricing, marketing, and distribution to improve business performance.

**Stakeholders' implications:**

Adidas Management - Insights from the project can help with effective
allocation of resources, with the purpose of maximization of
profitability and sales.

Retail Partners -- Improvement of forecasting the sales and
profitability might be of assistance to better plan the inventory and
foster collaborations with Adidas.

Customers - Indirectly benefit from optimized product availability and
potentially better pricing.

**Based on statistic analysis Adidas can:**

-improve sales techniques (location, product specific discounting,
better pricing strategy based on sales)

\- optimize marketing (find marketing targets better, find products that
fit the customers expectations better)

\- implement IoT technologies (for dynamic pricing strategy, for better
store experience - MobileApp)

Load important libraries:

```{r, echo=TRUE, results='hide', message=FALSE, warning=FALSE}
library(readxl)
library(rpart)
library(rpart.plot)
library(partykit)
library(ranger)
library(dplyr)
library(ggplot2)

```

Load in the data and take a first look:
(https://www.kaggle.com/datasets/heemalichaudhari/adidas-sales-dataset)

```{r, echo=TRUE, results='hide', message=FALSE, warning=FALSE}
df <- read_excel("Adidas US Sales Datasets.xlsx")
```

```{r}
head(df)
```

This output shows us that we have to do some data cleaning before
proceeding. We start by removing the first two rows, which only contain
NA values and continue by setting the new first row as the header and
removing it afterwards.

```{r}
df <- df[-c(1:2),] # remove first two rows
colnames(df) <- df[1,] # set first row as header
df <- df[-1,] # remove redundant first row

head(df) 
```

This looks already much better! The following code shows that there are
no more missing values!

```{r}
sum(is.na(df))
```

Now let us take a closer look at the structure of the data set.

```{r}
str(df)
```

We remove columns which are not useful for further analysis, such as
"Retailer ID", "Invoice Date" and "Operating Margin".

```{r}
df <- df[,-c(2,3,12)]
```

Now we just change the column names to make them easier to work with:

```{r}
colnames(df) <- c("Retailer", "Region", "State", "City", "Product", 
                  "Price_per_Unit", "Units_Sold", "Total_Sales", 
                  "Profit", "Sales_Method")
```

We continue by assigning the correct data type to each column:

```{r}
numcol <- c("Price_per_Unit", "Units_Sold", "Total_Sales", "Profit")

for (col in colnames(df)) {
  if (!col %in% numcol) {
    df[[col]] <- as.factor(df[[col]])
  } else {
    df[[col]] <- as.numeric(df[[col]])
  }
}

str(df)
```

Now let us examine a summary about the data:

```{r}
summary(df)
```

The high numbers in "Price_per_Unit", "Units_Sold", "Total_Sales" and
"Profit" might indicate outliers, so we use the IQR to find out how many
there might be. We are only dealing with financial columns when it comes
to outliers, so we decided to use a high boundary for determining the
thresholds (hence k = 2) because it is very likely that there are some
products which produced a very high profit or was sold a lot of times.

```{r}
find_outlier_count <- function(column) {
  Q1 <- quantile(column, 0.25)
  Q3 <- quantile(column, 0.75)  
  IQR <- Q3 - Q1                              
  
  lower_bound <- Q1 - 2 * IQR               
  upper_bound <- Q3 + 2 * IQR               
  
  outliers <- column[column < lower_bound | column > upper_bound]
  
  num_outliers <- length(outliers)
  
  return(list(
    Lower_Bound = lower_bound,
    Upper_Bound = upper_bound,
    Num_of_Outliers = num_outliers
  ))
}

# columns to analyze for outliers
columns_to_check <- c("Price_per_Unit", "Units_Sold", "Total_Sales", "Profit")

# apply the function and display results
outlier_results <- lapply(df[columns_to_check], find_outlier_count)

# Print the results for each column
for (column_name in columns_to_check) {
  cat("Results for", column_name, "\n")
  print(outlier_results[[column_name]][c("Lower_Bound", "Upper_Bound", "Num_of_Outliers")])
}
```

We now just remove those rows which exceed the upper thresholds,
removing 580 rows or 6% of the data set.

```{r}
# Extract the upper thresholds from the results
upper_thresholds <- sapply(outlier_results, function(result) result$Upper_Bound)

# Keep only rows that do not exceed the upper thresholds
dfx <- df[!apply(df[columns_to_check], 1, function(row) {
  any(row > upper_thresholds)
}), ]
```

Now then we have cleaned the data, we can focus on our first research
question:

1.  **What factors are the strongest predictors for profitability?**

We are interested in factors only and since it is obvious that the
profit is going to have a positive relationship with Units_Sold and
Total_Sales, we decided to exclude them as well as Price_per_Unit, since
it is also not a factor. However, we have to keep in mind that the R2
will therefore be much lower and that all the numeric variables are
always dominating trees and random forests in this particular data set!

```{r}
df1 <- dfx[,-c(6,7,8)]

str(df1)
```

Let us take a closer look at some visualizations:

```{r}
plot(Profit ~ ., data = df1)
```

Solely by looking at these visualizations, Sales_Method seems to have
very different effects on profit, with In-store being the most important
level. We now split the data into a training set and test set.

```{r}
set.seed(159)
idx <- sample(1:nrow(df1), size = 0.8 * nrow(df1), replace = FALSE)

trainData <- df1[idx, ]
testData <- df1[-idx, ]
```

Since our response is numerical, we fit a multiple linear regression on
the training data:

```{r}
train1 <- lm(Profit ~ ., data = df1)

summary(train1)
```

The output shows that each factor has at least some levels, which have a
significant impact on profit. Regarding Regions, South and Southeast
show the highest positive effect with no negative impacts. Cities as
well as states show positive and also some negative significant effects,
while the choice of the retailer does not seem to show a significant
effect. Regarding the available products, it seems that Men's Street
Footwear and Women's Apparel are the most important ones. Interestingly,
the methods of sale "Online" and "Outlet" have a highly significant and
have negative effect when compared to their baseline "In Store".

We now conduct a stepwise regression analysis:

```{r}
train2 <- step(train1, direction = "backward")

summary(train2)
```

It produces a very similar result, however it removes State as the only
column which is not kept. Furthermore, let us train a decision tree on
the training data:

```{r}
tree <- rpart(Profit ~ ., data = df1)

# plot(as.party(tree)) # the plotted tree is a mess, so we decided not to plot it by default

printcp(tree)
```

The tree makes use of City, Product, Sales_Method and State, excluding
Retailer and Region. Now it is time to prune the tree:

```{r}
plotcp(tree)
```

We use the 6th point from our diagram and use it to prune the tree:

```{r}
pruned_tree <- prune(tree, cp = tree$cptable[6, "CP"])
plot(as.party(pruned_tree))

printcp(pruned_tree)
```

Variables actually used in tree construction: City, Product,
Sales_Method, State

The tree sees these variables as important factors, as it splits on
them.

Let us now focus on the random forest, we are particularly interested in
variable importance, since we want to find out, which factors have the
highest effect on our response(Profit).

```{r}
rf <- ranger(Profit ~ ., data = df1, 
             probability = FALSE, 
             importance = "permutation")

print(rf)
```

And plot the resulting permutation importance:

```{r}
importance_values <- importance(rf)
importance_sorted <- sort(importance_values, decreasing = TRUE)
plot(as.table(importance_sorted), ylab = "Importance", las = 2,  cex.axis = 0.55)
```

As we can observe, Sales Method seems to be the most important factor
when it comes to predicting profit, this was backed by the pruned tree
too. Since we have now seen all the different results which depend on
the statistical method, we now want to perform a cross-validation and
find the method, which produces the lowest MSE.

```{r}
n <- nrow(df1)
fold <- 10
folds <- sample(rep(1:fold, ceiling(n/fold)), n)
# head(folds, n = 10)

mse <- list(tree = numeric(fold), rf = numeric(fold), lm = numeric(fold))  

for (tfold in seq_len(fold)) {
  # Split data into training and test sets
  train_idx <- which(folds != tfold)
  test_idx <- which(folds == tfold)
  
  # Fit models on the training set
  rf <- ranger(Profit ~ ., data = df1[train_idx, ], importance = "permutation")
  tree <- rpart(Profit ~ ., data = df1[train_idx, ], method = "anova")
  lm_model <- lm(Profit ~ ., data = df1[train_idx, ])

  # Make predictions on the test set
  p_rf <- predict(rf, data = df1[test_idx, ])$predictions
  p_tree <- predict(tree, newdata = df1[test_idx, ])
  p_lm <- predict(lm_model, newdata = df1[test_idx, ])

  # Calculate and store MSE
  actual <- df1$Profit[test_idx]
  mse$rf[tfold] <- mean((actual - p_rf)^2)
  mse$tree[tfold] <- mean((actual - p_tree)^2)
  mse$lm[tfold] <- mean((actual - p_lm)^2)
}

# Combine results into a data frame
mse_df <- data.frame(Tree = mse$tree, `Random Forest` = mse$rf, `Linear Regression` = mse$lm)

# Plot the cross-validated MSE
boxplot(mse_df, names = c("Tree", "Random Forest", "Linear Regression"),
        ylab = "Cross-validated Mean Squared Error",
        main = "Model Comparison: Cross-validated MSE")
```

As it is shown, the Random Forest outperforms the linear regression and
the tree. So we come to conclusion that the Sales_Method is the most
influential factor when it comes to the companies profit.

We now want to focus on our second research question:

2.  **Which sales channel is most effective for each product type?**

In order to answer this question, we also start by visualizing it by
first aggregating the data by Product and Sales_Method and using
Units_Sold as the reference:

```{r}
agg_data <- dfx %>%
  group_by(Product, Sales_Method) %>%
  summarize(Average_Sales = mean(Units_Sold), .groups = 'drop')

ggplot(agg_data, aes(x = Product, y = Average_Sales, fill = Sales_Method)) +
  geom_bar(stat = "identity", position = "dodge") + # Adjust width of dodge
  labs(title = "Average Sales by Product and Sales Method",
       x = "Product",
       y = "Average Sales") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) 
```

According to this simple bar chart, we observe that all products record
the highest average sales when sold in-store. Furthermore, "Men's Street
Footwear" has the highest average sales in all methods of sales. The
lowest average sales are recorded when sold online independent from the
chosen product.

Let us use produce another plot:

```{r}
agg_summary <- dfx %>%
  group_by(Product, Sales_Method) %>%
  summarize(
    Mean_Sales = mean(Units_Sold, na.rm = TRUE),
    Median_Sales = median(Units_Sold, na.rm = TRUE),
    SD_Sales = sd(Units_Sold, na.rm = TRUE),
    .groups = 'drop'
  )
print(agg_summary)

# Boxplot for visualizing sales distributions
ggplot(dfx, aes(x = Sales_Method, y = Units_Sold, fill = Sales_Method)) +
  geom_boxplot() +
  facet_wrap(~ Product) +
  labs(
    title = "Sales Distribution by Sales Channel and Product",
    x = "Sales Method",
    y = "Units Sold"
  ) +
  theme_minimal()
```

According to these boxplots, the most effective sales channel for each
product is In-Store.

Let us just fit a linear model on the data, using Units_Sold as the
response and Product as well as Sales_Method as the independent
variables.

```{r}
reg2 <- lm(Units_Sold ~ Product + Sales_Method, data = dfx)

summary(reg2)
```

The multiple linear regression model suggests, that both variables are
highly significant, and we also observe yet again that Online and Outlet
as the methods of sales perform significantly worse when compared to
their baseline, which is in-store. At this point we would like to find
out whether or not the effect of one variable on Units_Sold depends on
the level of the other variable. That is why we are fitting the data to
an interaction model:

```{r}
reg_interaction <- lm(Units_Sold ~ Product * Sales_Method, data = dfx)

summary(reg_interaction)
```

The intercept represents in this case the baseline Units_Sold for the
reference levels: When selling the baseline product (Men's Apparel)
using the baseline method (In-store), the expected Units_Sold is
approximately 274.54. The interaction effects, e.g. ProductMen's
Athletic Footwear:Sales_MethodOnline, represent how the combined effect
of Product and Sales_Method deviates from the additive effects of their
stand alone effects. This means, that when selling Men's Athletic
Footwear online, Units_Sold are decreased significantly more than
expected based on the individual effects of the product and sales method
alone. Such a highly significant and negative effect can also be found
by the following interactions:

ProductMen's Street Footwear:Sales_MethodOutlet ProductMen's Street
Footwear:Sales_MethodOnline ProductWomen's Apparel:Sales_MethodOnline

We will now focus on fitting a tree yet again, but this time we try to
use anova as its method:

```{r}

tree2 <- rpart(Units_Sold ~ Product + Sales_Method, data = dfx, method = "anova")

printcp(tree2)
```

Let us just plot the tree and look at the results:

```{r}
rpart.plot(tree2, main = "Decision Tree for Sales Effectiveness")
```

The tree suggests that the Sales Method increases the average Units_Sold
by almost 58% when selling in the stores and reduces it by almost 13%
when selling in online or in an outlet. The next two splits are
performed whether or not the sold product is "Men's Street Footwear". If
it is, the average Units_Sold greatly increases. So according to this
tree, the online method of sale as well as "Men's Street Footwear" show
a significant effect on the average units sold.

```{r}
rf2 <- ranger(Units_Sold ~ Product + Sales_Method, data = dfx, importance = "permutation")

print(rf2)
```

Plot the resulting permutation importance:

```{r}
importance <- importance(rf2)

print(importance)

importance_sorted <- sort(importance, decreasing = TRUE)
plot(as.table(importance_sorted), ylab = "Importance", las = 2, cex.axis = 0.55)
```

Once again, the variable Sales_Method is the more influential, now also
on the number of Units_Sold.

Let us now focus on the third and final question we want to answer:

3.  **How do different factors (product type, geographic region, price
    per unit)** **influence the number of units sold?**

We again look at some visualizations first:

```{r}
# Aggregate data
agg_data_product <- dfx %>%
  group_by(Product) %>%
  summarize(Average_Units_Sold = mean(Units_Sold), .groups = 'drop')

# Plot
ggplot(agg_data_product, aes(x = Product, y = Average_Units_Sold, fill = Product)) +
  geom_bar(stat = "identity") +
  labs(title = "Average Units Sold by Product Type",
       x = "Product Type",
       y = "Average Units Sold") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) 

# Aggregate data
agg_data_region <- dfx %>%
  group_by(Region) %>%
  summarize(Average_Units_Sold = mean(Units_Sold), .groups = 'drop')

# Plot
ggplot(agg_data_region, aes(x = Region, y = Average_Units_Sold, fill = Region)) +
  geom_bar(stat = "identity") +
  labs(title = "Average Units Sold by Region",
       x = "Region",
       y = "Average Units Sold") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) 


ggplot(dfx, aes(x = Price_per_Unit, y = Units_Sold)) +
  geom_point(alpha = 0.6) +
  geom_smooth(method = "lm", se = FALSE, color = "blue") +
  labs(title = "Units Sold vs. Price per Unit",
       x = "Price per Unit",
       y = "Units Sold") +
  theme_minimal()
```

We can observe, that "Men's Street Footwear" yields the most potential
to have a significant impact on the average Units_Sold. Regarding the
differences in regions, although Southeast shows the highest units sold,
there is not a big enough difference to assume a significant impact yet.
The last chart shows a slightly positive correlation between the Price
per Unit and the Units sold.

We now start by fitting a multiple linear regression model on the data:

```{r}
reg_model <- lm(Units_Sold ~ Product + Region + Price_per_Unit, data = dfx)

summary(reg_model)
```

The output shows us that a simple linear regression is definately not
enough to conduct an analysis, since it simply shows that all the
variables and different levels are highly significant (mostly positive),
combined with a very low R-squared value.

Let us once again use an interaction model:

```{r}
reg_interaction <- lm(Units_Sold ~ Product * Region * Price_per_Unit, data = dfx)

summary(reg_interaction)
```

Interestingly, the interaction model is a lot more picky when it comes
to the p-value. The most significant variables are ProductMen's Street
Footwear and Price_per_Unit, followed by ProductWomen's Apparel. Some
more interactions are significant, mainly:

ProductWomen's Street Footwear:RegionSoutheast:Price_per_Unit
ProductMen's Athletic Footwear:RegionSoutheast:Price_per_Unit
ProductMen's Street Footwear:Price_per_Unit ProductMen's Street
Footwear:RegionWest

If we just focus on ProductWomen's Street
Footwear:RegionSoutheast:Price_per_Unit, this can be interpreted, that
the effect of Price_per_Unit on Units_Sold depends on both the product
category (Women's Street Footwear) and the region (Southeast). In other
words, the relationship between price and units sold for "Women's Street
Footwear" is unique in the Southeast compared to other regions or
products.

..But the R-squared is still low, which can be explained by the fact
that we only consider the for us relevant factors, we could include more
but those variables wont help our question. Lets take a look on a
regression tree...

```{r}

tree3 <- rpart(Units_Sold ~ Product + Region + Price_per_Unit, data = dfx)

rpart.plot(tree3, main = "Decision Tree for Units Sold")
```

...and create a random forest:

```{r}
rf3 <- ranger(Units_Sold ~ Product + Region + Price_per_Unit, data = dfx, importance = "permutation")

print(rf3)
```

Now focus on the most important variables:

```{r}
importance <- importance(rf3)
importance_sorted <- sort(importance , decreasing = TRUE)
plot(as.table(importance_sorted), ylab = "Importance", las = 2, cex.axis = 0.55)
#plot(as.table(importance(rf3)), ylab = "Importance")
```

As we can observe, the Price per Unit has the most influence towards the
number of sold Units.

Now we want to perform a cross-validation and find the method, which
produces the lowest MSE.

```{r}
n <- nrow(df1)
fold <- 10
folds <- sample(rep(1:fold, ceiling(n/fold)), n)
# head(folds, n = 10)

mse <- list(tree = numeric(fold), rf = numeric(fold), lm = numeric(fold))  

for (tfold in seq_len(fold)) {
  # Split data into training and test sets
  train_idx <- which(folds != tfold)
  test_idx <- which(folds == tfold)
  
  # Fit models on the training set
  rf <- ranger(Units_Sold ~ Product + Region + Price_per_Unit, data = dfx[train_idx, ], importance = "permutation")
  tree <- rpart(Units_Sold ~ Product + Region + Price_per_Unit, data = dfx[train_idx, ], method = "anova")
  lm_model <- lm(Units_Sold ~ Product + Region + Price_per_Unit, data = dfx[train_idx, ])

  # Make predictions on the test set
  p_rf <- predict(rf, data = dfx[test_idx, ])$predictions
  p_tree <- predict(tree, newdata = dfx[test_idx, ])
  p_lm <- predict(lm_model, newdata = dfx[test_idx, ])

  # Calculate and store MSE
  actual <- dfx$Units_Sold[test_idx]
  mse$rf[tfold] <- mean((actual - p_rf)^2)
  mse$tree[tfold] <- mean((actual - p_tree)^2)
  mse$lm[tfold] <- mean((actual - p_lm)^2)
}

# Combine results into a data frame
mse_df <- data.frame(Tree = mse$tree, `Random Forest` = mse$rf, `Linear Regression` = mse$lm)

# Plot the cross-validated MSE
boxplot(mse_df, names = c("Tree", "Random Forest", "Linear Regression"),
        ylab = "Cross-validated Mean Squared Error",
        main = "Model Comparison: Cross-validated MSE")
```

The Random Forest outperforms the linear regression and the tree. So we
come to conclusion that the Price_per_unit is the most influential
factor when it comes to the number of units sold.

**Conclusion:**

Based on the findings from the first question, we recommend that retailers implement region-specific advertising and offer discounts for sales methods with lower performance. Our evaluation from the second question revealed that In-Store and Outlet Sales outperform Online Sales, suggesting that enhanced online marketing efforts and strategic discounting could positively impact sales performance. 
For the third question, we presented a geographical distribution plot, which can help refine advertising and discount strategies. Additionally, the decision tree analysis offers valuable insights into customer price sensitivity, aiding in more targeted decision-making. 
Combining all these findings: Adidas should focus on region-specific advertising and optimize underperforming sales channels, particularly by enhancing online marketing efforts and offering targeted discounts to boost e-commerce performance. Additionally, leveraging geographical distribution insights and understanding customer price sensitivity can help refine marketing strategies and maximize overall sales potential.
